<!doctype html>
<html lang="en">

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

	<title>Prelude</title>

	<link rel="stylesheet" href="dist/reset.css">
	<link rel="stylesheet" href="dist/reveal.css">
	<link rel="stylesheet" href="dist/theme/prelude.css">

	<!-- Theme used for syntax highlighted code -->
	<link rel="stylesheet" href="plugin/highlight/monokai.css">


	<link rel="preconnect" href="https://fonts.googleapis.com">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link
		href="https://fonts.googleapis.com/css2?family=Exo+2:ital,wght@0,100..900;1,100..900&family=Lato:ital,wght@0,100;0,300;0,400;0,700;0,900;1,100;1,300;1,400;1,700;1,900&display=swap"
		rel="stylesheet">

	<style>
		body,
		section {
			font-family: "Lato", sans-serif !important;
		}

		h1,
		h2,
		h3,
		h4,
		h5 {
			font-family: "Exo 2", sans-serif !important;

		}
	</style>

</head>

<body>
	<div
		style="position: absolute; bottom:0.5em; font-size:2em ; text-align: left; width: 100%; color: gray; padding-left: 0.5em;">
		prelude.tech | @sheena@fosstodon.org | https://sheenaoc.com | x: @sheena_oconnell
	</div>

	<div style="position: absolute; opacity:0.1; right: 7em; top: 2em">
		<img src="images/brand/Prelude - Leaf.svg" style="
		-moz-transform: scale(2.5);
    	-ms-transform: scale(2.5);
    	-o-transform: scale(2.5);
    	-webkit-transform: scale(2.5);
    	transform: scale(2.5);" />
	</div>

	<div style="position: absolute; opacity:0.1; left: 0em; bottom: 0em">
		<img src="images/brand/Prelude - Leaf.svg" style="
		-moz-transform: scale(1.5);
    	-ms-transform: scale(1.5);
    	-o-transform: scale(1.5);
    	-webkit-transform: scale(1.5);
    	transform: scale(1.5);" />
	</div>

	<div style="position: absolute; top:1em; left:1em">
		<img src="images/brand/Prelude - Logo.svg" width="250em" />
	</div>

	<div class="reveal">
		<div class="slides">

			<section>
				<h2>Links</h2>
				(presentation starts in next few slides)
				<ul>
					<li><a href="https://prelude.tech/">Prelude training</a></li>
					<li><a href="https://foxleytalent.com/">Foxley talent</a></li>
					<li><a href="https://guildofeducators.org/">Guild of educators</a></li>
					<li><a href="https://www.sheenaoc.com/">My blog + socials</a></li>
				</ul>
			</section>

			<!-- <section>
				<h2>Resources/references</h2>

			</section> -->

			<section>
				<h1>Playing the long game</h1>
				<h2 class="fragment">ü§ñ In the age of LLMs ü§ñ</h2>
			</section>

			<section>

				<h2>Things are pretty weird right now</h2>

				<ul>
					<li class="fragment">"AI" and LLMs are ubiquitous</li>
					<li class="fragment">Signal to noise ratio = terrible (hype + naysayers)</li>
					<!-- <li class="fragment">New tools and big news every other week</li> -->
					<li class="fragment">Emerging best practices</li>
					<li class="fragment">Emerging footguns</li>
					<li class="fragment">Economically concerning</li>
					<li class="fragment">Environmentally concerning</li>
				</ul>
			</section>

			

			<!-- <section>
				<h2>The job market is also weird</h2>
				<ul>
					<li class="fragment">Junior devs struggle to find work</li>
					<li class="fragment">Hiring unproven talent == risky</li>
					<li class="fragment">Low skilled people can't make a big impact <span class="fragment">...yet</span></li>
					<li class="fragment">Best case: They struggle to get XP</li>
					<li class="fragment">Worst case: They drop out completely</li>
				</ul>
			</section> -->

			<section>
				<h1>What will things look like in 5 years?</h1>
				<h2 class="fragment">How should we adapt?</h2>
			</section>

			<!-- <section>
				<img src="images/Terminator-2-judgement-day.jpg" height="500"/>
			</section> -->
			

<section>
				<h2>Hi, I'm Sheena</h2>
				<ul>
					<li class="fragment">Electrical engineer</li>
					<li class="fragment">Software eng and lead</li>
					<li class="fragment">
						Spent the last 6 years in tech education

						<ul>
							<li class="fragment">Software + "Soft" skills</li>
							<li class="fragment">Science of ed => engineering
								of ed</li>
						</ul>
					</li>
					<li class="fragment">Founded Prelude.Tech</li>
					<li class="fragment">Founded Guild of Educators </li>
					<li class="fragment">PyCon Africa 2025 Chair</li>
					<li class="fragment">PSF Board member</li>
				</ul>
				<br/>
				<span class="fragment">
					üßó‚Äç‚ôÄÔ∏èüèïÔ∏èüß≠üáøüá¶üñäÔ∏èüõ†Ô∏èüî•üêïüé∏üë©üèª‚Äçüíª üßë‚Äçüè´
				</span>
			</section>

			<section>
				<h2>
					LLMs and education
				</h2>
				<ul>
					<li class="fragment">Preparing people for the future is my whole focus</li>
					<li class="fragment">Lots of people ask me for career advice   </li>
				</ul>
				<h1 class="fragment">üò®</h1>
			</section>

		
			
			
			<section>
				<h2>What are we doing here?</h2>
				<ul>
					<li class="fragment">My latest thinking</li>
					<!-- <li class="fragment">Education xp 
						<li class="fragment">
Software engineering xp
						</li>
						
						<li class="fragment">Experimentation</li>
					<li class="fragment">Research</li>
					<li class="fragment">Chats with clever friends</li> -->
					<li class="fragment">Not the final word</li>
					<!-- <li class="fragment">Encurage you</li> -->
					<!-- <li class="fragment">We need to talk</li> -->
				</ul>
</section>



			<!-- <section>
				<h1>Other people making hard decisions</h1>
				<ul>
					<li class="fragment">Experienced devs </li>
					<li class="fragment">Organizations that write code</li>
				</ul>
			</section> -->

			

			<section>
				<h2>Likely futures</h2>
				<!-- <p class="fragment"></p> -->
				<ul>
					<li class="fragment">Super-intelligence explosion</li>
					<li class="fragment">The bubble bursts</li>
					<li class="fragment">AI capabilities stabilize</li>
				</ul>
			</section>

			

			<section>
				<h1>Part 1</h1>
				<h1>Looking back to look forward</h1>
			</section>

			<section>
				<h2>A brief history of LLMs</h2>
				<ul >
						<li class="fragment">2018: GPT 1</li>
						<li class="fragment">2019: GPT 2 (post-training/fine tuning)</li>
						<li class="fragment">2020: GPT 3</li>
					<li class="fragment">2020: RAG</li>
					<li class="fragment">2021: Anthropic</li>
					<li class="fragment">2021: Codex + Github Copilot</li>
					<li class="fragment">December 2022: ChatGPT</li>
				</ul>
			</section>

			

			<section>
				<h2>Scaling "laws"</h2>
				<ul>
					<li class="fragment">More data => More intelligence</li>
					<li class="fragment">More parameters => More intelligence</li>
					<li class="fragment">More money => More intelligence</li>
				</ul>
			</section>

			<section>
				<h2>New capabilities emerged with scale</h2>

				<ul>
					<li class="fragment">Capabilities that seem to appear suddenly at certain scales</li>
					<li class="fragment">Mathematical reasoning at 100 Billion parameters</li>
					<li class="fragment">In-context learning (the ability to learn from examples within a single conversation) appeared suddenly</li>
					<li class="fragment">But some abilities showed "inverse scaling"‚Äîthey actually got worse as models got larger</li>
				</ul> 
			</section>
			
			<section>
				<h2>2022: Deepmind Chinchilla paper</h2>
				<ul>
					<li class="fragment">Chinchilla = 70-billion-parameter model on 1.4 trillion tokens</li>
					<li class="fragment">GPT-3 = 175 billion parameters</li>
					<li class="fragment">Gopher = 280 billion parameters</li>
					<li class="fragment">Chinchilla out-performed these while MUCH smaller</li>
					<li class="fragment">Estimates suggested the internet contained about 500 trillion tokens of unique text</li>		
									<!-- https://www.jonvet.com/blog/llm-scaling-in-2025  -->
				</ul>
			</section>

			<section>
				<h2>Not all tokens are created equal</h2>
				<ul>
					<li class="fragment">Quality data is hard to find</li>
					<li class="fragment">Low hanging fruit has been picked</li>
				</ul>
			</section>

			
			<section>
				<h2>2023</h2>
				<ul>
					<li class="fragment">OpenAI Plugin ecosystem + functions</li>
					<li class="fragment">GPT-4 was amazing</li>
					<li class="fragment">Microsoft research paper titled ‚ÄúSparks of Artificial General Intelligence.‚Äù</li>
					<li class="fragment">Venture-capital spending on A.I. jumped by eighty per cent in following year</li>
					<li class="fragment">OpenAI's launches "Superalignment" project (Ilya Sutskever)</li>
				</ul>
			</section>

			<section>
				<h2>Post training</h2>
				<ul>
					<!-- <li class="fragment">Pre-training = eat the Internet</li>
					<li class="fragment">Post-training = eat opinionated data</li> -->
					<li class="fragment">Alignment - helpful, truthful, harmless</li>
					<li class="fragment">Task/domain customization</li>
					<li class="fragment">Open AI post-training since GPT-2</li>
					<li class="fragment">Where do post-training datasets come from?</li>
				</ul>
			</section>

			<section>
				<h2>ü§ó <a href="https://huggingface.co/models?sort=trending&search=prescription">search=prescription</a></h2>
				<img src="images/hugging-face.png" alt="" height="500">
			</section>

			<!-- <section>
				<h2>Winner takes all?</h2>

				<ul>
					<li class="fragment">Who wants to use the second-best model?</li>
					<li class="fragment">Race to add resources = race to raise money</li>
					
					<li class="fragment">Race to capture users</li>
					<li class="fragment">Hype fuels that fire</li>

				</ul>
			</section> -->

			<section>
				<h2>2024: Plot twist üçì</h2>

				<ul>
					<li class="fragment">Open AI releases o1. A reasoning model.</li>
					<li class="fragment">!= GTP4 + üí∏üí∏üí∏</li>
					<li class="fragment">!= GPT5</li>
					<li class="fragment">Iterative, chain-of-thought reasoning</li>
					<li class="fragment">Come up with multiple answers, evaluate, return best one (scale up thinking time)</li>
					<li class="fragment">December: o3 released</li>
				</ul>
			</section>

			<section>
				<h2>Late 2024: MCP</h2>
				
				<p>Open standard for connecting any AI assistant (LLM) to any external tool or data source in a plug-and-play way</p>

			</section>

			<section>
				<h2>2024: New age of discovery</h2>
				<p>

					"The 2010s were the age of scaling, now we're back in the age of wonder and discovery once again. Everyone is looking for the next thing" 
				</p>
				<p>- Ilya Sutskever </p>
				<p style="font-size:large;">OpenAI co-founder | Safe Superintelligence Inc co founder</p>
			</section>





			<section>
				<h2>2025: Plot twist</h2>

					<img src="images/DeepSeek_logo.svg" style="padding-top:1em;" class="fragment" alt="">
				<!-- <ul>
					<li class="fragment">January: DeepSeek R1</li>
					<li class="fragment">Performance comparable to big models</li>
					<li class="fragment">But much more resource efficient</li>
				</ul> -->
			</section>

			<section>
				<h2>Deepseek benchmarks</h2>
				<img src="images/deepseek-bench.png" alt="">
			</section>

			<section>
				<h2>Training cost</h2>
				<img src="images/deepseek-training-cost.png" alt="">
			</section>
			<section>
				<h2>Deepseek mixture of experts</h2>

				<img src="images/deepseek-moe.png" alt="">
			</section>

			
			

			





<!-- <section>
				<h2>Model Explosion</h2>
				<ul>
					<li class="fragment">Weights are just numbers. Capabilities opaque</li>
					<li class="fragment">No view of training / post-training data</li>
					<li class="fragment">Need to trust quality of work</li>
					<li class="fragment">Need to trust intentions</li>
				</ul>
			</section>

			
			<section>
				<h2>Job market</h2>
			</section> -->

						<!-- <section>
				<h2>Model explosion</h2>
				https://huggingface.co/models?search=prescription

				GPT5 isn't a model 
				- has real time router
				- a few models that the router calls on as needed
				- minimizing sycophancy

				See how it does on various benchmarks: https://openai.com/index/introducing-gpt-5/
			</section>
			
			<section>Scaling thinking time TODO</section>

<section>Different techniques
	Chain of thought 
	Breaking problems down 
	Different agent prompts having different roles 
	Using internet to get most up to date info, instead of needing to know everything
	MoE models
</section>
!-->
<!-- 
			<section>
				<h2>Scale matters</h2>
				<ul>
					<li class="fragment">Pre-training datasets</li>
					<li class="fragment">Parameter count</li>
					<li class="fragment"></li>
				</ul>
			</section> -->


			<section>
				<h2>Not just scale</h2>
				<ul>
					<li class="fragment">2024: o1, o3 => different model architectures</li>
					<li class="fragment">Jan 2025: R1 => new architecture</li>
				</ul>
			</section>

			

			
			<section>
				<h2>Feb 2025</h2>

				<p>

					"The intelligence of an AI model roughly equals the log of the resources used to train and run it. These resources are chiefly training compute, data, and inference compute. It appears that you can <strong>spend arbitrary amounts of money and get continuous and predictable gains;</strong> the scaling laws that predict this are accurate over many orders of magnitude."
				</p>
					<p>
						- Sam Altman
					</p>

			</section>

			
			<section>
				<h2>Deepseek economic effect</h2>
				<ul>
					<li class="fragment">NVIDIA dropped 17% in one day</li>
					<li class="fragment">$593 billion</li>
					<li class="fragment">Biggest single day loss in US market history</li>
					<li class="fragment">Previous record holder ... <span class="fragment">NVIDIA!</span></li>
				</ul>
			</section>

			<section>
				<h2>2025: January. Stargate announced</h2>
				<ul>
					<li class="fragment">$500 Billion AI infrastructure company</li>
					<li class="fragment">1 GW could power approximately 876,000 American households</li>
					<!-- <li class="fragment">July: Oracle and OpenAI agree to add extra 4.5 gigawatts</li> -->
					<li class="fragment">15-20 Gw total. A small nation</li>
					<li class="fragment">Plans to expand to other countries. Stargate Norway announced in July</li>
					<li class="fragment">Commitment to clean energy. Requires production, transmission and storage innovations</li>
					<!-- <li class="fragment">
						Support AI developments in medical diagnostics and climate prediction, among other
					</li> -->
					<!-- <li class="fragment">Small modular nuclear power sources</li> -->
				</ul>
			</section>

			<section>
				<h2>XAI Colossus cluster</h2>

				<ul>
					<li class="fragment">Hosting 100,000 H100s</li>
					<li class="fragment">Plans to expand this to at least 1 million</li>
				</ul>


				<!-- https://www.jonvet.com/blog/llm-scaling-in-2025#are-scaling-laws-still-valid-or-have-we-hit-a-wall -->
			</section>

			
			

			

			



			<!-- <section>
				<h2>2025: May/June</h2>
				"Theoretically, at some point, you can see that a significant fraction of the power on Earth should be spent running AI compute. And maybe we're going to get there." - Sam Altman
			</section> -->

		

			<section>
				<h2>2025 July. GPT 5</h2>
				<ul>
					<li class="fragment">Generally underwhelming</li>
					<li class="fragment">Required different prompting strategy <a href="https://gist.github.com/maoxiaoke/f6d5b28f9104cd856a2622a084f46fd7">(leak)</a></li>
					<li class="fragment">Under the hood: router that selects between multiple models</li>
				<li class="fragment">In many cases, performance comparable with 03</li>
				</ul>
				<!-- https://openai.com/index/introducing-gpt-5/ -->
				
			</section>

			<!-- <section>
				<h2>History - main takeaways</h2>
				<ul>
					<li class="fragment">Scaling laws held pretty well for a while</li>
					<li class="fragment">Post training gives an edge (also, more specialist models)</li>
					<li class="fragment">Model architecture matters!</li>
					<li class="fragment">Orchistration and tooling matter (engineering)</li>
					<li class="fragment">Rise of the Vibe</li>
					<li class="fragment">Spinning hype is good business</li>
				</ul>
			</section> -->

			<!-- <section>
				<h2>Hype means...</h2>
				<ul>
					<li class="fragment">Businesses struggle to make good decisions</li>
					<li class="fragment">Early career devs aren't sure if there will be work</li>
					<li class="fragment">Arms race in hiring - CV slop and imitations, versus automated screening</li>
					<li class="fragment">Everyone is a junior</li>
				</ul>
			</section> -->

			<section>
				<h1>Part 2</h1>
				<h2>Looking to the future</h2></section>

				<section>
				<h2>Likely futures</h2>
				<!-- <p class="fragment"></p> -->
				<ul>
					<li >Super-intelligence explosion</li>
					<li >The bubble bursts</li>
					<li >AI capabilities stabilize</li>
				</ul>
			</section>

			<section>
				<h2>Timeline 1: <br/>Super-intelligence explosion</h2>

				<ul>
					<li class="fragment">Current trajectory => AGI</li>
					<li class="fragment">Assumes scaling laws hold</li>
					<li class="fragment">üí∏üí∏üí∏ => AGI</li>
					<li class="fragment">Different model architectures yield different results - have we landed on an architecture that is capable of AGI?</li>
				</ul>
			</section>						

			<section>
				<h2>Are scaling laws really "laws"</h2>

				<ul>
					<li class="fragment">No answer about why scaling works. Foundational theory missing</li>
					<li class="fragment">Hardware costs are massive</li>
					<li class="fragment">Physical resource limitations</li>
					<li class="fragment">Hard limits in data for pre-training (510B tokens. Not same quality)</li>
					<li class="fragment">

						largest known dataset is ~18T tokens (Qwen2.5)
					</li>
					<li class="fragment">Post training data requires value judgments</li>
				</ul>


<!-- https://www.jonvet.com/blog/llm-scaling-in-2025#are-scaling-laws-still-valid-or-have-we-hit-a-wall -->

			</section>

			<!-- <section><h2>It it an S-curve?</h2>
			
			TODO: slowing down
			</section> -->

			
			<!-- <section>
				<h2>Most likely: reinforcement learning to make specialized models</h2>
				<ul>
					<li class="fragment">Start with a base model</li>
					<li class="fragment">Collect preference feedback (A is better than B)</li>
					<li class="fragment">Stir</li>
				</ul>
			</section> -->

			<!-- <section>
				<h2>Model explosion</h2>
				https://huggingface.co/models?search=prescription

				GPT5 isn't a model 
				- has real time router
				- a few models that the router calls on as needed
				- minimizing sycophancy

				See how it does on various benchmarks: https://openai.com/index/introducing-gpt-5/
			</section>
			
			<section>Scaling thinking time TODO</section>

<section>Different techniques
	Chain of thought 
	Breaking problems down 
	Different agent prompts having different roles 
	Using internet to get most up to date info, instead of needing to know everything
	MoE models
</section> -->





<section>
	<h2>What are the chances of AGI? Really?</h2>

	<ul>
		<li class="fragment">No compelling evidence, only inference</li>
		<li class="fragment">Winner takes all</li>
		<li class="fragment">Clear that techniques and architectures are not in optimal state</li>
	
	</ul>
</section>


<section>

	<p>"I‚Äôve seen the story happen for enough times to really believe that probably the scaling is going to continue, and that there‚Äôs some magic to it that we haven‚Äôt really explained on a theoretical basis yet."</p>

		<p>- 
			Dario Amodei of Anthropic
		</p>
</section>

<section>
<p>

	"There is no wall"
</p>
<p>
	- Sam Altman
</p>
</section>

<!-- 
			
 

TODO

https://www.jonvet.com/blog/llm-scaling-in-2025#are-scaling-laws-still-valid-or-have-we-hit-a-wall
-->






			<section>
				<h2>Timeline 2: The bubble bursts</h2>
			</section>

			<section>
				<h2>Investment versus profit</h2>
				<ul>
						<li class="fragment">2024
							<ul>
								<li>
									OpenAI lost $5 billion</li>

								</li>
								<li>Anthropic lost $5.3 billion</li>
							</ul>
						<!-- https://www.wheresyoured.at/why-everybody-is-losing-money-on-ai/ -->
					<li class="fragment">2025: OpenAI‚Äôs annual recurring revenue is now on track to pass $20 billion this year, still losing money</li>
				</ul>
				<!-- https://www.cnbc.com/2025/08/08/chatgpt-gpt-5-openai-altman-loss.html -->
			</section>

			<section>
				<h2>Volatility in market</h2>
				<ul>
					<li class="fragment">January 2025, NVIDIA set the record for most money lost in a day (~ $600 Billion)</li>
					<li class="fragment">They held the previous record </li>
				</ul>
			</section>

			<section>
				<h2>Magnificent 7</h2>
				<p class="fragment">~ 35% of US stock market</p> 
				<ul class="fragment">
					<li>NVIDIA</li>
					<li>Microsoft</li>
					<li>Apple</li>
					<li>Alphabet(Google)</li>
					<li>Amazon</li>
					<li>Meta</li>
					<li>Tesla</li>
				</ul>
			</section>

			<section>
				<h2>Going deeper</h2>
				<ul>
					<li class="fragment">NVIDIA makes up 19% of magnificent 7 market value</li>
					<li class="fragment">42.4% of NVIDIA revenue = Microsoft, Amazon, Meta, Alphabet, Tesla</li>
					<li class="fragment">Companies like Coreweave and Crusoa ~ 10% Nvidia revenue</li>
					<li class="fragment">Meta spends 25% of capital expendatures on NVIDIA chips</li>
					<li class="fragment">Microsoft spends 47% of capital expendatures on NVIDIA chips</li>
				</ul>
			</section>

			<!-- <section>
				<h2>Financial investment</h2>

				<ul>
					<li class="fragment">x Billion Dollars invested in 2024</li>
					<li class="fragment">Who is profitable?</li>
					<li class="fragment">Was ChatGPT 5 released in a rush?</li>
				</ul>
				
			</section> -->

			<section>
				<h2>What happens if the bubble bursts?</h2>
				<ul>
					<li class="fragment">Can't use big financial hammers to make LLMs more powerful any more</li>
					<li class="fragment">Existing infrastructure wont go away</li>
					<li class="fragment">Core capabilities stabilize</li>
					<li class="fragment">Tools become more expensive? Maybe</li>
				</ul>
			</section>
			
			<section>
				<h2>Reverse brain drain</h2>
				<ul>
					<li class="fragment">ML experts and academics working in "closed" companies (big paychecks)</li>
					<li class="fragment">Talent released to the market</li>
					<li class="fragment">Talent available to other orgs</li>
					<li class="fragment">Talent fed back to academia => open research</li>
				</ul>
			</section>

			<!-- <section>
				<h2>How likely is this?</h2>
				<ul>
					<li class="fragment">Many valid reasons to worry</li>
					<li class="fragment">LLMs are already very useful</li>
					<li class="fragment">Stargate investments are huge</li>
				</ul>
			</section> -->

			<section>
				<h2>Timeline 3: LLM capabilities stabilize</h2>
			</section>

			<section>
				<h2>Comparisons</h2>
				<ul>
					<li class="fragment">Seems more likely than a super-intelligence explosion</li>
					<li class="fragment">If the bubble bursts, this will happen</li>
					<li class="fragment">So plan for this one!</li>
				</ul>
			</section>

			<section>
				<h2>But aren't there new AI tools like every 5 minutes?</h2>
				
				
				
				<ul>
					<li class="fragment">Post-training makes existing models good at different things</li>
					<li class="fragment">LLMs are components of these tools, not whole new models</li>
				</ul>

			</section>

			<section>
				<h2>Job market</h2>
				<ul>
					<li class="fragment">Mass layoffs</li>
					<li class="fragment">5 year low</li>
				</ul>
			</section>
			

			<section>
				<h2>Job market crash</h2>
				<!-- <ul>
					<li class="fragment">TODO + standford experiment</li>
				</ul> -->

				<img src="images/fredgraph.png"/>
				<p>US Job postings on Indeed</p>
				<!-- https://blog.pragmaticengineer.com/software-engineer-jobs-five-year-low/#:~:text=Facts%20about%20software%20developer%20jobs,on%20Indeed -->
				<!-- https://newsletter.pragmaticengineer.com/p/state-of-the-tech-market-in-2025 -->
			</section>

			<section>
				<h2>What happened?</h2>
				<ul>
					<li class="fragment">Recession => layoffs</li>
					<li class="fragment">Covid => level up remote products/services</li>
					<li class="fragment">Zero interest rates => Free money</li>
					<li class="fragment">Flood of new talent</li>
					<li class="fragment">Company right-sizing => low open positions</li>
					<li class="fragment">Hesitance to make that mistake again</li>
					<li class="fragment">Emphasis on effective teams rather than big teams</li>
				</ul>
			</section>


			<section>
				<h2>Recovery</h2>
				<img src="images/trueup.png" alt="">

			</section>

			<section>
				<h2>What's happening?</h2>
				<ul>
					<li class="fragment">Natural attrition => job openings</li>
					<li class="fragment">Vibe code => new opportunities
						<ul>
							<li class="fragment">Vibe code cleanup specialist</li>
							<li class="fragment">More entrepreneurs</li>
						</ul>
					</li>
					<li class="fragment">New jobs: AI-security, observability, ops...</li>
				</ul>
			</section>

			
			<!-- <section>
				<h2>Junior dev jobs</h2>
				<ul>
					<li class="fragment">Internal pipelines</li>
					<li class="fragment"></li>
				</ul>
			</section> -->
			
			<section>
				<h2>Predictions</h2>
				<ul>
					<li class="fragment">Model capabilities will not make major jumps. Might get a faster car, but not a rocket-ship</li>
					<li class="fragment">Explosion of model applications will continue</li>
					<li class="fragment">Serious problems (eg medical) will be owned by big players</li>
					<!-- <li class="fragment">Likely innovations in clean energy, but not sure about water</li> -->
				</ul>
			</section>
			

			<section>
				<h1>Part 3</h1>
				<h2>Adapting to a likely future</h2>
				
			</section>

			<section>
				<h2>Different strokes</h2>
				<ul>
					<li class="fragment">Early-career devs</li>
					<li class="fragment">Established developers and engineers</li>
					<li class="fragment">Organizations</li>
				</ul>
			</section>

			<section>
				<h2>Early career developers</h2>
				<ul>
					<li class="fragment">Opportunities are scarce right now
						<ul>
							<li class="fragment">Some jobs automated</li>
							<li class="fragment">Talent pool = huge and experienced</li>
						</ul>
					</li>
					<li class="fragment">A lot of people dropping out of tech entirely</li>
					<!-- <li class="fragment">Likely future talent vacuum - if you get good there will be work</li> -->
					<li class="fragment">Unsure of what skills to build</li>
					<li class="fragment">Hard to get noticed</li>
					<li class="fragment">LLMs can help or harm</li>
				</ul>
			</section>

			<section>
				<h2>Don't give up</h2>
				<ul>
					<li class="fragment">There is likely to be a talent vacuum</li>
					<li class="fragment">Get good!</li>
					<li class="fragment">Get noticed!</li>
				</ul>
			</section>

			

			<section>
				<h2>Anti-patterns</h2>
				<ul>
					<li class="fragment">Tutorial hell</li>
					<li class="fragment">Shallow knowledge of many tools</li>
					<li class="fragment">Portfolio of tiny projects</li>
					<li class="fragment">Chasing certificates</li>
					<!-- <li class="fragment">Undervalue understanding</li>
					<li class="fragment">Prioritizing fanciness and half-baked features</li> -->
					<!-- <li class="fragment">Cargo-culting</li> -->
				</ul>
			</section>

			<section>
				<h2>Build fundamental skills</h2>
				<ul>
					<li class="fragment">Code literacy is necessary</li>
					<li class="fragment">Use katas to build problem solving skills</li>
					<li class="fragment">Read about best practices</li>
				</ul>
			</section>

			<section>
				<h2>Go deep by building big</h2>
				<ul>
					<li class="fragment">Build + demonstrate mental discipline, grit, attention to detail</li>
					<li class="fragment">Steer clear of golden pathways</li>
					<li class="fragment">Use LLMs to write code but you will hit a wall</li>
					<li class="fragment">When you get stuck - that is a trigger to learn more</li>
					<li class="fragment">Simple architecture</li>
					<li class="fragment">One COMPLETE feature at a time -> understand iteratively</li>
					<li class="fragment">Be your own vibe-code cleanup specialist</li>
				</ul>
			</section>

			<!-- <section>
				<h2>What this gives you</h2>
				<ul>
					<li class="fragment">Validate your work</li>
					<li class="fragment">DRY, cohesive, loosely coupled code</li>
					<li class="fragment">Avoid cargo-culting</li>
					<li class="fragment">Avoid illusions of competence</li>
				</ul>
			</section> -->

			<section>
				<h2>Get noticed</h2>
				<ul>
					<li class="fragment">Avoid CV-padding, CV spam, sloppy cover letters</li>
					<li class="fragment">Build a reputation by showing up and adding value</li>
					<li class="fragment">Human interactions and relationships</li>
				</ul>
			</section>



			<section>
				<h2>Established devs and engineers</h2>
				<ul>
					<li class="fragment">LLMs give a productivity boost in many (not all) situations</li>
					<li class="fragment">Be aware of your blast radius</li>
					<li class="fragment">Validation and testing</li>
					<li class="fragment">Observability</li>
					<li class="fragment">Use tools that keep context simple and consistent</li>
					<li class="fragment">Use tools with guardrails built in</li>
					<!-- <li class="fragment">Communicate carefully</li> -->
				</ul>
			</section>

			<!-- <section>
				<h2>Security tri-factor</h2>
				<ul>
					<li class="fragment">Private data</li>
				</ul>
			</section> -->


			<section>
				<h2>Organizations</h2>
				<ul>
					<li class="fragment">Don't be fooled by greenfield POCs</li>
					<li class="fragment">Ask your most capable and experienced engineers to find ways to use the tools safely</li>
					<li class="fragment">Favor deterministic systems</li>
					<!-- <li class="fragment">Security and privacy best practices not fully understood or developed</li> -->
					<!-- <li class="fragment">Focus on guardrails, validation and observability</li> -->
					<!-- <li class="fragment"></li> -->
					<!-- <li class="fragment">LLM based products are slow, expensive and non-deterministic</li> -->
				</ul>
			</section>


			<section>
				<h2>Future talent</h2>
				<ul>
					<li class="fragment">Build a grad program/internship program</li>
					<li class="fragment">Avoid the CV arms race</li>
					<li class="fragment">Focus on relationships and community</li>
					<li class="fragment">"Recruiter" doesn't need to be a dirty word</li>
				</ul>
			</section>

			<!-- <section>
				<h2>Nurture talent pipeline</h2>
				<ul>
					<li class="fragment">Grad program?</li>
				</ul>
			</section> -->

			<section>
				<h2>Nurturing the dev ecosystem and talent pool</h2>
				<ul>
					<li class="fragment">Invest in the ecosystem</li>
					<li class="fragment">Not a zero sum game</li>
					<li class="fragment">Build a good reputation</li>
					<li class="fragment">More talent, partners, clients, ideas, opportunities</li>
				</ul>
			</section>

			<section>
				<h2>More...</h2>
				<ul>
					<li>Talent</li>
					<li>Partners</li>
					<li>Clients</li>
					<li>Innovation</li>
					<li>Opportunities</li>
				</ul>
			</section>

			<section>
				<h1>It's an investment</h1>
			</section>

			<section>
				<h2>Thank you</h2>

				<img src="qr.png" alt="" height="400">
			</section>

			<!-- <section>
				<h2>Derisk hiring</h2>
			</section> -->











			
			
			
			

			





		</div>
	</div>

	<script src="dist/reveal.js"></script>
	<script src="plugin/notes/notes.js"></script>
	<script src="plugin/markdown/markdown.js"></script>
	<script src="plugin/highlight/highlight.js"></script>
	<script>
		// More info about initialization & config:
		// - https://revealjs.com/initialization/
		// - https://revealjs.com/config/
		Reveal.initialize({
			hash: true,

			// Learn about plugins: https://revealjs.com/plugins/
			plugins: [RevealMarkdown, RevealHighlight, RevealNotes]
		});
	</script>
</body>

</html>