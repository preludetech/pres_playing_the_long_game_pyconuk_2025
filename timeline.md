Timeline


# 2023

https://www.newyorker.com/culture/open-questions/what-if-ai-doesnt-get-much-better-than-this
- March, 2023, OpenAI’s next release, GPT-4, vaulted so far up the scaling curve that it inspired a Microsoft research paper titled “Sparks of Artificial General Intelligence.”
- Over the following year, venture-capital spending on A.I. jumped by eighty per cent.





# 2024

## Feb 
https://fortune.com/2024/02/12/sam-altman-7-trillion-ai-chips-grind-for-future-substack/
- Sam Altman wants to raise up to $7 trillion for a new AI chip project
- more than the entire federal budget, twice the U.K.’s annual GDP, 13 times 2023’s global chip sales, or enough to pay for over two years of universal health care in the U.S.
- “The amount of natural resources that will be required is just mind-boggling,” she told VentureBeat. “Even if the energy is renewable (which it isn’t guaranteed to be), the quantity of water and rare earth minerals required is astronomical.”

# Sept
https://openai.com/index/learning-to-reason-with-llms/

o1 way more powerful than GPT4
. We have found that the performance of o1 consistently improves with more reinforcement learning (train-time compute) and with more time spent thinking (test-time compute). The constraints on scaling this approach differ substantially from those of LLM pretraining
-  graphs show that o1 is better almost all of the time, but not in all domains


## Nov
https://www.tiktok.com/@todayin_ai/video/7440122892022451457
The new version is that there is not one scaling law, but three: scaling with how long you train a model (which isn’t really holding anymore), scaling with how long you post-train a model, and scaling with how long you let a given model wrestle with a given problem (or what Satya Nadella called scaling with “inference time compute”).




# 2025 

## January

Deepseek R1 
https://www.nbcnews.com/data-graphics/deepseek-ai-comparison-openai-chatgpt-google-gemini-meta-llama-rcna189568 
- image of cost of training 

Stargate: $500B infrastructure company
- planning to expland in other countries
- Stargate Norway was then launched on July 31, 2025 
- collaboration is hard
- July: Oracle and OpenAI have entered an agreement to develop 4.5 gigawatts of additional Stargate data center capacity in the U.S.https://openai.com/index/stargate-advances-with-partnership-with-oracle/
- 1 GW could power approximately 876,000 american households for one year https://www.carboncollective.co/sustainable-investing/gigawatt-gw


## Feb 

https://blog.samaltman.com/three-observations
- Feb 2025 Sam altman was still saying that more money more ai:
 1. The intelligence of an AI model roughly equals the log of the resources used to train and run it. These resources are chiefly training compute, data, and inference compute. It appears that you can spend arbitrary amounts of money and get continuous and predictable gains; the scaling laws that predict this are accurate over many orders of magnitude.
2. The cost to use a given level of AI falls about 10x every 12 months, and lower prices lead to much more use. You can see this in the token cost from GPT-4 in early 2023 to GPT-4o in mid-2024, where the price per token dropped about 150x in that time period. Moore’s law changed the world at 2x every 18 months; this is unbelievably stronger. 
3. The socioeconomic value of linearly increasing intelligence is super-exponential in nature. A consequence of this is that we see no reason for exponentially increasing investment to stop in the near future.



## May/June

"Theoretically, at some points, you can see that a significant fraction of the power on Earth should be spent running AI compute," Altman said. "And maybe we're going to get there."




Aug 2025: GPT 5 released

https://www.newyorker.com/culture/open-questions/what-if-ai-doesnt-get-much-better-than-this
- In an Ask Me Anything (A.M.A.) session, Altman and other OpenAI engineers found themselves on the defensive, addressing complaints. Marcus summarized the release as “overdue, overhyped and underwhelming.”
- _The Information_ reported, “the increase in quality was far smaller compared with the jump between GPT-3 and GPT-4.”
- But these changes now feel narrow—more like the targeted improvements you’d expect from a software update than like the broad expansion of capabilities in earlier generative-A.I. breakthroughs...You didn’t need a bar chart to recognize that GPT-4 had leaped ahead of anything that had come before


When???
https://www.newyorker.com/culture/open-questions/what-if-ai-doesnt-get-much-better-than-this
- In June XXX Apple researchers released a paper titled “The Illusion of Thinking,” which found that state-of-the-art “large reasoning models” demonstrated “performance collapsing to zero” when the complexity of puzzles was extended beyond a modest threshold...o3-mini, Claude 3.7 Sonnet’s “thinking” mode, and DeepSeek-R1, “still fail to develop generalizable problem-solving capabilities,
- When Grok 3 failed to outperform its competitors significantly, the company embraced post-training approaches to develop Grok 4.